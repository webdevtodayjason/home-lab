{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "example-md-1",
      "metadata": {},
      "source": [
        "# vLLM GPU Inference Example\n",
        "\n",
        "This notebook demonstrates how to use vLLM for high-performance LLM inference on your RTX 4090."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU status\n",
        "import torch\n",
        "import subprocess\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
        "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory // (1024**3) if torch.cuda.is_available() else 0} GB\")\n",
        "\n",
        "# Show nvidia-smi\n",
        "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "print(\"\\nNVIDIA-SMI Output:\")\n",
        "print(result.stdout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "import-vllm",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import vLLM\n",
        "from vllm import LLM, SamplingParams\n",
        "import time\n",
        "\n",
        "print(\"vLLM imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-model",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a small model for demonstration\n",
        "# You can replace this with larger models like \"meta-llama/Llama-2-7b-chat-hf\" if you have access\n",
        "\n",
        "print(\"Loading model...\")\n",
        "llm = LLM(\n",
        "    model=\"microsoft/DialoGPT-small\",  # Small model for demo\n",
        "    trust_remote_code=True,\n",
        "    gpu_memory_utilization=0.5,  # Use 50% of GPU memory\n",
        "    max_model_len=512  # Limit sequence length\n",
        ")\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inference",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sampling parameters\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    max_tokens=100\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "prompts = [\n",
        "    \"Hello, how are you today?\",\n",
        "    \"What is artificial intelligence?\",\n",
        "    \"Explain machine learning in simple terms.\",\n",
        "    \"What are the benefits of GPU acceleration?\"\n",
        "]\n",
        "\n",
        "print(f\"Generating responses for {len(prompts)} prompts...\\n\")\n",
        "\n",
        "# Measure inference time\n",
        "start_time = time.time()\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "end_time = time.time()\n",
        "\n",
        "# Display results\n",
        "for i, output in enumerate(outputs):\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt {i+1}: {prompt}\")\n",
        "    print(f\"Response: {generated_text}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(f\"\\nTotal inference time: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Average time per prompt: {(end_time - start_time) / len(prompts):.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "batch-inference",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate batch processing benefits\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Test different batch sizes\n",
        "batch_sizes = [1, 2, 4, 8]\n",
        "times = []\n",
        "\n",
        "test_prompt = \"Tell me about the future of AI.\"\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    prompts_batch = [test_prompt] * batch_size\n",
        "    \n",
        "    start_time = time.time()\n",
        "    outputs = llm.generate(prompts_batch, sampling_params)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    avg_time_per_prompt = (end_time - start_time) / batch_size\n",
        "    times.append(avg_time_per_prompt)\n",
        "    \n",
        "    print(f\"Batch size {batch_size}: {avg_time_per_prompt:.3f}s per prompt\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(batch_sizes, times, 'bo-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Batch Size')\n",
        "plt.ylabel('Average Time per Prompt (seconds)')\n",
        "plt.title('vLLM Batch Processing Performance on RTX 4090')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nBest performance at batch size {batch_sizes[np.argmin(times)]} with {min(times):.3f}s per prompt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated:\n",
        "1. GPU detection and status checking\n",
        "2. vLLM model loading and configuration\n",
        "3. Single and batch inference\n",
        "4. Performance measurement and optimization\n",
        "\n",
        "### Next Steps:\n",
        "- Try larger models like Llama 2 7B or 13B\n",
        "- Experiment with different sampling parameters\n",
        "- Implement streaming responses for real-time applications\n",
        "- Explore quantization for memory efficiency"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}